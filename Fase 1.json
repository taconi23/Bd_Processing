{"paragraphs":[{"text":"// Primer paso importar las librerias necesarias\nimport spark.implicits._  // Importa todo el paquete implicits\nimport spark.sql","user":"anonymous","dateUpdated":"2019-02-17T19:47:08+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import spark.implicits._\nimport spark.sql\n"}]},"apps":[],"jobName":"paragraph_1550346886175_-1961289081","id":"20190216-205446_1160768413","dateCreated":"2019-02-16T20:54:46+0100","dateStarted":"2019-02-17T19:47:08+0100","dateFinished":"2019-02-17T19:47:08+0100","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:8356"},{"text":"// Voy a cargar los datos como un dataSet por lo tanto lo primero es crear la clase correspondiente\n\ncase class Piso_ft(MLS: Integer,\n                location: String,\n                price: Double,\n                Bedrooms: Integer,\n                Bathrooms: Integer,\n                Size: Double,\n                `Price SQ Ft`: Double,  //al tener espacios en blanco hay que encerrar el nombre entre ``\n                Status: String)","user":"anonymous","dateUpdated":"2019-02-17T19:47:08+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"defined class Piso_ft\n"}]},"apps":[],"jobName":"paragraph_1550346908969_410111368","id":"20190216-205508_1875777489","dateCreated":"2019-02-16T20:55:08+0100","dateStarted":"2019-02-17T19:47:08+0100","dateFinished":"2019-02-17T19:47:08+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:8357"},{"text":"// configuro el path de entrada y salida así como el fichero a cargar. Esto es por comodidad para no tener que estar renonbrando fichero etc ya que configura tambien el \n// nombre del fichero de salidad  que se llamara igual pero con extensión json. Hay que tener cuidado de no meter dos veces el mismo fichero sino da un error de que el ficehro ya existe\nval path = \"file:///home/kc/Documentos/datasets/\"\nval path_salida = \"file:///home/kc/Documentos/datasets/real-estate/\"\n\n// el nombre va sin extensión ya que se añade despues\nval fichero = \"entrada5\"","user":"anonymous","dateUpdated":"2019-02-17T19:48:46+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"path: String = file:///home/kc/Documentos/datasets/\npath_salida: String = file:///home/kc/Documentos/datasets/real-estate/\nfichero: String = entrada5\n"}]},"apps":[],"jobName":"paragraph_1550428827244_-1974844630","id":"20190217-194027_2026923291","dateCreated":"2019-02-17T19:40:27+0100","dateStarted":"2019-02-17T19:47:08+0100","dateFinished":"2019-02-17T19:47:08+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:8358"},{"text":"// Ahora hay que cargar los datos del csv proporcionado\nval datosDS =  spark.read\n            .option(\"sep\", \",\")   // Especifico el separador que son comas\n            .option(\"header\", true)   // Que importe la cabecera\n            .option(\"inferSchema\", true)     // Infiere el tipo de dato a partir de la clase\n            .csv(path + fichero + \".csv\").as[Piso_ft]   //Tipo de fichero csv y clase Piso para crear el dataSet","user":"anonymous","dateUpdated":"2019-02-17T19:47:09+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"datosDS: org.apache.spark.sql.Dataset[Piso_ft] = [MLS: int, Location: string ... 6 more fields]\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://10.0.2.15:4041/jobs/job?id=41","http://10.0.2.15:4041/jobs/job?id=42"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1550346932593_-1041725135","id":"20190216-205532_920081805","dateCreated":"2019-02-16T20:55:32+0100","dateStarted":"2019-02-17T19:47:09+0100","dateFinished":"2019-02-17T19:47:09+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:8359"},{"text":"/*\nAhora voy a realizar las transformaciones para que cambie los pies cuadrado por metro cuadrado, dolares a euros y el precio por unidad de area proporcionado\nen $/ft^2 a €/m^2. Los factore de conversión son 1$ = 1.13€ y 1m^2 = 10.764 ft^2. Además solo me quedo con las columnas relevantes que son MLS, la localización, precio,  area del piso y precio por metro cuadrado.\n\nComo quiero obtener otro dataSet debo crear la clase correspondiente\n*/\ncase class Piso_m2 (MLS: Integer,\n                location: String,\n                price: Double,\n                Size: Double,\n                `PriceSQM2`: Double  //al tener espacios en blanco hay que encerrar el nombre entre ``\n                )","user":"anonymous","dateUpdated":"2019-02-17T19:47:09+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"defined class Piso_m2\n"}]},"apps":[],"jobName":"paragraph_1550346987073_1205746949","id":"20190216-205627_1549949719","dateCreated":"2019-02-16T20:56:27+0100","dateStarted":"2019-02-17T19:47:09+0100","dateFinished":"2019-02-17T19:47:09+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:8360"},{"text":"// realizo la conversión\nval precios_euros = datosDS.map(precio => (precio.MLS, precio.location, precio.price/1.13, precio.Size/10.764, precio.`Price SQ Ft`*10.764/1.13)).toDF(\"MLS\", \"location\", \"price\", \"Size\", \"PriceSQM2\").as[Piso_m2]","user":"anonymous","dateUpdated":"2019-02-17T19:47:09+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"precios_euros: org.apache.spark.sql.Dataset[Piso_m2] = [MLS: int, location: string ... 3 more fields]\n"}]},"apps":[],"jobName":"paragraph_1550346990776_-2129815887","id":"20190216-205630_29688407","dateCreated":"2019-02-16T20:56:30+0100","dateStarted":"2019-02-17T19:47:09+0100","dateFinished":"2019-02-17T19:47:09+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:8361"},{"text":"// realizo la agrupación de los datos del precio por metro cuadrado por localidad\nval ag_location = precios_euros.groupBy(\"Location\").agg(avg(\"PriceSQM2\"))","user":"anonymous","dateUpdated":"2019-02-17T19:47:09+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"ag_location: org.apache.spark.sql.DataFrame = [Location: string, avg(PriceSQM2): double]\n"}]},"apps":[],"jobName":"paragraph_1550347023559_-1767145582","id":"20190216-205703_538494792","dateCreated":"2019-02-16T20:57:03+0100","dateStarted":"2019-02-17T19:47:09+0100","dateFinished":"2019-02-17T19:47:10+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:8362"},{"text":"// escribo el resultado a disco a la carpeta real-estate que habrá que monitorizar en la fase 2.\n     ag_location.coalesce(1).write.json(path_salida + fichero + \".json\")","user":"anonymous","dateUpdated":"2019-02-17T19:47:10+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"org.apache.spark.sql.AnalysisException: path file:/home/kc/Documentos/datasets/real-estate/entrada5.json already exists.;\n  at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:114)\n  at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n  at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n  at org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n  at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\n  at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\n  at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:668)\n  at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:668)\n  at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n  at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:668)\n  at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:276)\n  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:270)\n  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:228)\n  at org.apache.spark.sql.DataFrameWriter.json(DataFrameWriter.scala:536)\n  ... 81 elided\n"}]},"apps":[],"jobName":"paragraph_1550347077653_1645927931","id":"20190216-205757_2027518297","dateCreated":"2019-02-16T20:57:57+0100","dateStarted":"2019-02-17T19:47:10+0100","dateFinished":"2019-02-17T19:47:10+0100","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:8363"},{"user":"anonymous","dateUpdated":"2019-02-17T19:45:54+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1550347095556_1937592748","id":"20190216-205815_69514792","dateCreated":"2019-02-16T20:58:15+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:8364"}],"name":"Fase 1","id":"2E4X7SPTD","noteParams":{},"noteForms":{},"angularObjects":{"spark:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}